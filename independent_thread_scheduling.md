NVIDIA CUDA, since the Volta architecture, guarantees that the following code makes forward progress: 
(The example on the right is from https://ieeexplore.ieee.org/document/8344474.) 
In the CUDA’s previous execution model the critical section might never get executed as the architecture assumed it can safely wait until all (active) threads exit the while-loop first before proceeding to the next region. In this case, only one CUDA thread exits the region at a time, thus a deadlock can occur if threads are not allowed independent progress outside the loop (the thread which acquired the mutex can never release it). With Volta, when a thread calls atomic operations, it is detected by the thread scheduler which then schedules threads from other predicate regions to allow execution to proceed outside the loop. This is a correctness issue with kernels such as the above which require independent thread progress. 
This is a major execution model change since in previous versions threads (work-items) could be assumed to be independent of each other (not sharing data that affects semantics) unless using the local memory and barrier synchronization which allowed certain control flow optimizations. Now it’s possible to collaborate and share data also between threads using atomic variables.  
This picture from https://developer.nvidia.com/blog/inside-volta/ explains the change in CUDA from Volta. In the previous model, the whole if-branch (in the above example, the while) was executed (A; B;) by all enabled threads before the else until the end, now the thread scheduler can interleave execution as shown, thus guaranteeing progress for the example case above (if A or B is an atomic operation, it schedules work-items from the another branch). 
In OpenCL 2.x execution model: “The work-items within a single work-group execute concurrently but are only guaranteed to make independent progress in the presence of sub-groups and device support. In the absence of this capability, only high-level synchronization constructs (e.g. work-group functions such as barriers) that apply to all the work-items in a work-group are well defined and included in OpenCL for  synchronization within the work-group.” 
Supporting this execution model semantics doesn’t seem to be ongoing in Khronos. We could possibly support some cases when the device provides sub-group forward progress and we can have subgroups of size 1, but this is likely not realistic with current implementations. 
Level Zero doesn’t seem to specify a separate execution model for the device programs, and is likely implicitly following the OpenCL/SPIR-V one. Thus, supporting this feature requires a new OpenCL extension. For example, a per-device property which states that “Forward progress is guaranteed when utilizing control-flow affecting atomic variables.” There’s a similar property for subgroups, here we’d expand it to cover all work-items in the work group and perhaps tie it to data shared via atomic operations and perhaps limit its effect only to cases that affect work-item control flow (proving that a case doesn’t affect control flow could be left to the compiler’s predicate analysis). 
Efficient hardware support for this type of kernels is an entirely different story. E.g. PoCL’s implicit work-group vectorization won’t work for this case as this construct basically converts “lock step control flow” to dynamic control flow, thus we have to resort to fiber-style execution or a similar dynamic means to execute the WIs instead of static mapping to SIMD lanes. 
Fallback CPU execution could be implemented: Usage of atomics should indicate the possible need for independent forward progress guarantees. The detection can be optimized by catching only cases that affect control flow decisions. Basic lock-free algorithms that use atomics for data updates (not for mutexes) would then not be affected and might be still supported without the forward progress guarantees and could be even statically work-group vectorized in many cases. 
Independent thread scheduling is not supported by HIP at all. 